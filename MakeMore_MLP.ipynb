{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "names = open('names.txt').read().splitlines()\n",
    "import random\n",
    "random.seed(512)\n",
    "random.shuffle(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = sorted(list(set(''.join(names))) + ['.'])\n",
    "n_unique_chars = len(chars)\n",
    "n_unique_chars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {character:index for index,character in enumerate(chars)}\n",
    "stoi['.']\n",
    "itos = {index:character for character,index in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['muzammil',\n",
       " 'jamieson',\n",
       " 'kaylina',\n",
       " 'devi',\n",
       " 'jazlene',\n",
       " 'khaidyn',\n",
       " 'dakota',\n",
       " 'paiden',\n",
       " 'kendrik']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "random.seed(500)\n",
    "random.shuffle(names)\n",
    "[names[i] for i in (torch.arange(-10,-1)).tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182560, 5]) torch.Size([182560])\n",
      "torch.Size([22862, 5]) torch.Size([22862])\n",
      "torch.Size([22724, 5]) torch.Size([22724])\n"
     ]
    }
   ],
   "source": [
    "block_size = 5 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  for w in words:\n",
    "\n",
    "    #print(w)\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      #print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape,Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "\n",
    "n1 = int(0.8*len(names))\n",
    "n2 = int(0.9*len(names))\n",
    "\n",
    "Xtr, Ytr = build_dataset(names[:n1])\n",
    "Xval, Yval = build_dataset(names[n1:n2])\n",
    "Xte, Yte = build_dataset(names[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28421"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_neurons = 200\n",
    "vecembedding_dim = 22\n",
    "\n",
    "W1 = torch.randn((vecembedding_dim *block_size,hidden_neurons)) \n",
    "#b1 = torch.randn((hidden_neurons))\n",
    "W2 = torch.randn((hidden_neurons,n_unique_chars))\n",
    "b2 = torch.randn(n_unique_chars)\n",
    "C = torch.randn((n_unique_chars,vecembedding_dim))\n",
    "batchnorm_gain = torch.ones(size=[1,hidden_neurons])\n",
    "batchnorm_bias = torch.zeros(size=[1,hidden_neurons])\n",
    "\n",
    "# Individual stats are to keep track of these metrics when validation split \n",
    "# is being tested, because then batchnorm is not used, hence we need a global mean and std  \n",
    "# ALso, these are not learnable parameters, they will be calculated under torch.no_grad()\n",
    "# decoration to isolate it from the computation graph, since it is just a variable updation, \n",
    "# see training code to understand.\n",
    "\n",
    "individual_mean = torch.zeros(size=[1,hidden_neurons])\n",
    "individual_std = torch.ones(size=[1,hidden_neurons])\n",
    "\n",
    "parameters = [C,W1,W2,b2,batchnorm_gain,batchnorm_bias]\n",
    "for params in parameters:\n",
    "    params.requires_grad = True\n",
    "    \n",
    "y = sum(p.nelement() for p in parameters)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 98/200000 [00:00<06:30, 511.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0/200000 -  loss:18.1969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 10067/200000 [00:19<05:24, 584.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10000/200000 -  loss:2.2580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 20055/200000 [00:45<06:55, 433.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 20000/200000 -  loss:2.5357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 30052/200000 [01:10<08:34, 330.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 30000/200000 -  loss:2.2721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 40060/200000 [01:34<06:11, 430.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 40000/200000 -  loss:2.5676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 50051/200000 [01:59<06:09, 405.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 50000/200000 -  loss:2.1879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 60057/200000 [02:24<05:28, 425.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 60000/200000 -  loss:2.1692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 70067/200000 [02:43<03:49, 566.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 70000/200000 -  loss:2.0701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 80078/200000 [03:01<03:35, 555.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 80000/200000 -  loss:2.3395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 90082/200000 [03:18<03:11, 573.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 90000/200000 -  loss:1.9042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 100064/200000 [03:36<03:21, 496.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100000/200000 -  loss:2.1106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 110079/200000 [03:54<02:34, 582.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 110000/200000 -  loss:2.0542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 120084/200000 [04:11<02:18, 577.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 120000/200000 -  loss:2.3263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 130098/200000 [04:29<02:04, 560.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 130000/200000 -  loss:2.4381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 140084/200000 [04:47<01:42, 584.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 140000/200000 -  loss:2.2697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 150082/200000 [05:04<01:30, 550.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 150000/200000 -  loss:2.2785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 160088/200000 [05:22<01:08, 582.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 160000/200000 -  loss:2.0066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 170061/200000 [05:39<00:51, 581.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 170000/200000 -  loss:2.2836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 180094/200000 [05:57<00:34, 573.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 180000/200000 -  loss:2.1049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 190107/200000 [06:15<00:17, 573.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 190000/200000 -  loss:2.3371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200000/200000 [06:32<00:00, 509.62it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "#learning_rates_shop = torch.linspace(0.001,2.000,2000)\n",
    "#losses = []\n",
    "#learning_rates = []\n",
    "minibatch_size = 64\n",
    "\n",
    "for i in tqdm(range(200000)):\n",
    "    #Forward Pass\n",
    "    mini_batch_indices = torch.randint(0,Xtr.shape[0],(minibatch_size,)) \n",
    "    embedding = C[Xtr[mini_batch_indices]]\n",
    "    h_preactivation = embedding.view(minibatch_size,block_size*vecembedding_dim) @ W1 #+b1\n",
    "    batch_mean = h_preactivation.mean(dim=0,keepdim=True)\n",
    "    batch_std = h_preactivation.std(dim=0,keepdim=True)\n",
    "    h_preactivation = ((h_preactivation - batch_mean) * (batchnorm_gain))/batch_std  + batchnorm_bias\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        individual_mean = individual_mean*0.99 + batch_mean*0.01\n",
    "        individual_std  = individual_std *0.99 + batch_std *0.01\n",
    "    \n",
    "    h = torch.nn.functional.tanh(h_preactivation)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = torch.nn.functional.cross_entropy(logits,Ytr[mini_batch_indices])\n",
    "    \n",
    "    \n",
    "    #Backward Pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    #update\n",
    "    lr = 0.1 if i<60000 else 0.01\n",
    "    for params in parameters:\n",
    "        params.data += -lr * params.grad\n",
    "    #losses.append(loss.item())\n",
    "    #learning_rates.append(learning_rates_shop[i])\n",
    "    \n",
    "    if (not i%10000):\n",
    "        print(f\"epoch: {(i)}/200000 -  loss:{loss.item():.4f}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val 2.1617770195007324\n",
      "train 2.161390781402588\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def split_loss(split):\n",
    "    \n",
    "    x,y = {\n",
    "    'train': (Xtr, Ytr),\n",
    "    'val': (Xval, Yval),\n",
    "    'test': (Xte, Yte),\n",
    "          }[split]\n",
    "      \n",
    "    #Forward Pass\n",
    "    embedding = C[x]\n",
    "    h_preactivation = embedding.view(embedding.shape[0],-1) @ W1 #+b1\n",
    "\n",
    "    h_preactivation = ((h_preactivation - individual_mean)*batchnorm_gain /individual_std) + batchnorm_bias\n",
    "    \n",
    "    h = torch.tanh(h_preactivation)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = torch.nn.functional.cross_entropy(logits,y)\n",
    "    print(split,loss.item())\n",
    "\n",
    "split_loss('val')\n",
    "split_loss('train')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.plot(learning_rates_shop,losses)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppqqvihnzvqzqdhqupypogxfogdzlqqavikahufgkuffprxmgdgwivjqsvivqdawhwrvwvkwyvk.\n",
      "pprpquvibquqqqupyuguzubgurxcqsshfovdfrququvahcdkthftplbmdoxdmwaddzkqqqevddgxwvismwyvquzwzcelmgrqxtismbqdagdovdigmavddgqupzpulbhcdossepzlchbyuchofgfvprvegocktrffrymmgwqwivqtpwvigzzydqucwhmgyvun.\n",
      "ppqpvibgeldjgh.\n",
      "ppqqvihnzvqzqdhqququbkdigszpfppfpfpvffdpqqdveqcpypo.\n",
      "ppqqvysqupwvifzeymmucquqohghgcpjfprusqopllbgwyengzqqqzvinghqx.\n",
      "ppqqvihnzhfpvjvqukthqd.\n",
      "ppqqve.\n",
      "ppqqvysqupwvugozufgpuffprxmgdgwivmavdwghwvwaxfwrlqvanhymdagquqdhwagnxfjvapqjkecqsuklagdogxnviqzjqmaghqugwvcdozvigquptpugblbygqupwpugzwrtffprvmgrxkcifzevdzqxyvqupwygqufwpcfffrvqmawysqdpwptwissppolqdan.\n",
      "ppqpvbhwfwdvcvengzuszzqxpvigmousqupypogxuffprxmgdgwivqagddgqupzpulbhcxtvifmiyzmahqdaghufzzxpzlly.\n",
      "brvllogh.\n"
     ]
    }
   ],
   "source": [
    "# predictions\n",
    "\n",
    "for i in range(10):\n",
    "    out = []\n",
    "    context = [0] * block_size \n",
    "    while True:\n",
    "      emb = C[torch.tensor([context])] \n",
    "      h = torch.tanh(emb.view(1, -1) @ W1 )\n",
    "      logits = h @ W2 + b2\n",
    "      probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "      ix = torch.multinomial(probs, num_samples=1, generator=None).item()\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      if ix == 0:\n",
    "        break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
